{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f475c45-54cb-4bea-8b51-0f7a792a3b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "save_directory = r'C:\\Users\\Jaber\\OneDrive - University of Florida\\Educational\\Chapters\\Chapter1_Abstain\\Codes\\Results\\Results_Transformer_Autoencoder'\n",
    "\n",
    "# Define the range of lambda values\n",
    "lambdas = np.arange(0.5, 0.95, 0.05)\n",
    "\n",
    "# Initialize dictionaries to store metrics for each lambda\n",
    "metrics_dict = {l: {'accuracy': [], 'precision': [], 'recall': [], 'f1': [], 'specificity': []} for l in lambdas}\n",
    "\n",
    "# Function to classify based on probability and threshold\n",
    "def classify_with_reject(probabilities, threshold, initial_fp_fn_indices):\n",
    "    predictions = []\n",
    "    abstain_instances = []  # To store indices of abstain instances\n",
    "    for i, prob in enumerate(probabilities):\n",
    "        if i not in initial_fp_fn_indices:\n",
    "            if prob >= 0.5:\n",
    "                predictions.append(1)\n",
    "            else:\n",
    "                predictions.append(0)\n",
    "        else:\n",
    "            if prob >= threshold:\n",
    "                predictions.append(1)  # Classify as positive if prob is >= threshold\n",
    "            elif prob < 1 - threshold:\n",
    "                predictions.append(0)  # Classify as negative if prob is < 1 - threshold\n",
    "            else:\n",
    "                predictions.append(-1)  # Reject classification for uncertain predictions\n",
    "                abstain_instances.append(i)  # Record abstain instance index\n",
    "    return np.array(predictions), abstain_instances\n",
    "\n",
    "\n",
    "# Load new dataset\n",
    "new_data_path = r\"C:\\Users\\Jaber\\OneDrive - University of Florida\\Educational\\Chapters\\Chapter1_Abstain\\Datasets\\ShandsData\\combined_BPM_data.csv\"\n",
    "\n",
    "# Load the new data\n",
    "new_df = pd.read_csv(new_data_path)\n",
    "\n",
    "# Store instance names\n",
    "instance_names = new_df.iloc[:, 0].values  # Save the first column as instance names\n",
    "\n",
    "# Remove the first column which contains non-numeric data (names of instances)\n",
    "new_df = new_df.iloc[:, 1:]\n",
    "\n",
    "# Convert labels to binary (1 or 2 to 0 or 1)\n",
    "new_df['label'] = new_df['label'].apply(lambda x: 0 if x == 1 else 1)\n",
    "\n",
    "# Treat missing values with mean strategy only for numeric columns\n",
    "numeric_columns = new_df.select_dtypes(include=[np.number]).columns\n",
    "new_df[numeric_columns] = new_df[numeric_columns].fillna(new_df[numeric_columns].mean())\n",
    "\n",
    "# Extract features (FHR time series) and labels\n",
    "X_new_time_series = new_df.iloc[:, :-1].values  # Exclude the label column\n",
    "y_new = new_df['label'].values  # Include the target variable y\n",
    "\n",
    "# Autoencoder for Dimensionality Reduction\n",
    "input_dim = X_new_time_series.shape[1]\n",
    "encoding_dim = 30\n",
    "\n",
    "input_layer = tf.keras.layers.Input(shape=(input_dim,))\n",
    "encoder = tf.keras.layers.Dense(128, activation=\"relu\")(input_layer)\n",
    "encoder = tf.keras.layers.Dense(64, activation=\"relu\")(encoder)\n",
    "encoder = tf.keras.layers.Dense(encoding_dim, activation=\"relu\")(encoder)\n",
    "decoder = tf.keras.layers.Dense(64, activation='relu')(encoder)\n",
    "decoder = tf.keras.layers.Dense(128, activation='relu')(decoder)\n",
    "decoder = tf.keras.layers.Dense(input_dim, activation='sigmoid')(decoder)\n",
    "\n",
    "autoencoder = tf.keras.models.Model(inputs=input_layer, outputs=decoder)\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train the Autoencoder\n",
    "autoencoder.fit(X_new_time_series, X_new_time_series, epochs=50, batch_size=32, shuffle=True, verbose=0)\n",
    "\n",
    "# Encode the data\n",
    "encoder_model = tf.keras.models.Model(inputs=input_layer, outputs=encoder)\n",
    "X_new_time_series_reduced = encoder_model.predict(X_new_time_series)\n",
    "\n",
    "# Leave-One-Out Cross Validation\n",
    "loo = LeaveOneOut()\n",
    "\n",
    "fold_number = 1\n",
    "for train_index, test_index in loo.split(X_new_time_series_reduced):\n",
    "    X_train_new, X_test_new = X_new_time_series_reduced[train_index], X_new_time_series_reduced[test_index]\n",
    "    y_train_new, y_test_new = y_new[train_index], y_new[test_index]\n",
    "\n",
    "    # Standardize the features (mean=0, std=1)\n",
    "    scaler_time_series_new = StandardScaler()\n",
    "    X_train_new_time_series = scaler_time_series_new.fit_transform(X_train_new)\n",
    "    X_test_new_time_series = scaler_time_series_new.transform(X_test_new)\n",
    "\n",
    "    # Define the Transformer model\n",
    "    def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "        # Normalization and Attention\n",
    "        x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
    "        x = tf.keras.layers.MultiHeadAttention(\n",
    "            key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
    "        )(x, x)\n",
    "        x = tf.keras.layers.Dropout(dropout)(x)\n",
    "        res = x + inputs\n",
    "\n",
    "        # Feed Forward Part\n",
    "        x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(res)\n",
    "        x = tf.keras.layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
    "        x = tf.keras.layers.Dropout(dropout)(x)\n",
    "        x = tf.keras.layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
    "        return x + res\n",
    "\n",
    "    def build_model(input_shape, head_size, num_heads, ff_dim, num_transformer_blocks, mlp_units, dropout=0, mlp_dropout=0):\n",
    "        inputs = tf.keras.Input(shape=input_shape)\n",
    "        x = inputs\n",
    "        for _ in range(num_transformer_blocks):\n",
    "            x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
    "\n",
    "        x = tf.keras.layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
    "        for dim in mlp_units:\n",
    "            x = tf.keras.layers.Dense(dim, activation=\"relu\")(x)\n",
    "            x = tf.keras.layers.Dropout(mlp_dropout)(x)\n",
    "        outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "        return tf.keras.Model(inputs, outputs)\n",
    "\n",
    "    input_shape = (X_train_new_time_series.shape[1], 1)\n",
    "    transformer_model = build_model(\n",
    "        input_shape,\n",
    "        head_size=256,\n",
    "        num_heads=4,\n",
    "        ff_dim=4,\n",
    "        num_transformer_blocks=4,\n",
    "        mlp_units=[128],\n",
    "        dropout=0.25,\n",
    "        mlp_dropout=0.4,\n",
    "    )\n",
    "\n",
    "    transformer_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Train the model with early stopping\n",
    "    best_model_new = None\n",
    "    best_score_new = 0\n",
    "    no_improvement_epochs_new = 0\n",
    "    patience_new = 3  # Patience for early stopping\n",
    "\n",
    "    for epoch in range(20):  # Number of epochs\n",
    "        history_new = transformer_model.fit(X_train_new_time_series, y_train_new, epochs=1, batch_size=8, validation_split=0.1, verbose=0)\n",
    "\n",
    "        # Evaluate on the training data\n",
    "        train_accuracy_new = history_new.history['accuracy'][-1]\n",
    "        \n",
    "        # Check for early stopping\n",
    "        if train_accuracy_new > best_score_new:\n",
    "            best_model_new = transformer_model\n",
    "            best_score_new = train_accuracy_new\n",
    "            no_improvement_epochs_new = 0\n",
    "        else:\n",
    "            no_improvement_epochs_new += 1\n",
    "        \n",
    "        if no_improvement_epochs_new >= patience_new:\n",
    "            print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "            break\n",
    "\n",
    "    # Use the best model for predictions\n",
    "    test_probabilities_new = best_model_new.predict(X_test_new_time_series).flatten()\n",
    "\n",
    "    # Initial classification with lambda=0.5\n",
    "    initial_predictions_new = (test_probabilities_new >= 0.5).astype(int)\n",
    "    cm_initial_new = confusion_matrix(y_test_new, initial_predictions_new, labels=[0, 1])\n",
    "    tn_initial_new, fp_initial_new, fn_initial_new, tp_initial_new = cm_initial_new.ravel()\n",
    "\n",
    "    initial_fp_fn_indices_new = [i for i in range(len(initial_predictions_new)) if initial_predictions_new[i] != y_test_new[i]]\n",
    "\n",
    "    # Initialize lists to store confusion matrix elements\n",
    "    tp_list_new = []\n",
    "    tn_list_new = []\n",
    "    fp_list_new = []\n",
    "    fn_list_new = []\n",
    "\n",
    "    # Initialize a table to store results for each lambda\n",
    "    table_data_new = []\n",
    "    abstain_table_data_new = []\n",
    "    metrics_table_data_new = []\n",
    "\n",
    "    # Name of the tested instance in this fold\n",
    "    tested_instance_name = instance_names[test_index[0]]\n",
    "    print(f\"Tested Instance in Fold {fold_number}: {tested_instance_name}\")\n",
    "\n",
    "    # Loop through lambda values and calculate metrics\n",
    "    for reject_threshold_new in lambdas:\n",
    "        predictions_new, abstain_indices_new = classify_with_reject(test_probabilities_new, reject_threshold_new, initial_fp_fn_indices_new)\n",
    "        \n",
    "        # Filter out abstained instances\n",
    "        filtered_indices_new = [i for i in range(len(predictions_new)) if predictions_new[i] != -1]\n",
    "        y_test_filtered_new = y_test_new[filtered_indices_new]\n",
    "        predictions_filtered_new = predictions_new[filtered_indices_new]\n",
    "        \n",
    "        # Calculate confusion matrix elements\n",
    "        if len(predictions_filtered_new) > 0:\n",
    "            cm_new = confusion_matrix(y_test_filtered_new, predictions_filtered_new, labels=[0, 1])\n",
    "            tn_new, fp_new, fn_new, tp_new = cm_new.ravel()\n",
    "        else:\n",
    "            cm_new = np.array([[0, 0], [0, 0]])\n",
    "            tn_new, fp_new, fn_new, tp_new = 0, 0, 0, 0\n",
    "\n",
    "        # Append confusion matrix elements to lists\n",
    "        tp_list_new.append(tp_new)\n",
    "        tn_list_new.append(tn_new)\n",
    "        fp_list_new.append(fp_new)\n",
    "        fn_list_new.append(fn_new)\n",
    "        \n",
    "        # Append data to table\n",
    "        table_data_new.append([round(reject_threshold_new, 2), tn_new, fp_new, fn_new, tp_new])\n",
    "        \n",
    "        # Report abstain instances (rows of data)\n",
    "        abstain_instances_info_new = []\n",
    "        for idx_new in abstain_indices_new:\n",
    "            abstain_instances_info_new.append((idx_new, y_test_new[idx_new]))\n",
    "        \n",
    "        abstain_table_data_new.append([round(reject_threshold_new, 2), abstain_instances_info_new])\n",
    "\n",
    "        # Calculate and store metrics\n",
    "        if len(y_test_filtered_new) > 0:\n",
    "            accuracy_new = accuracy_score(y_test_filtered_new, predictions_filtered_new) * 100\n",
    "            precision_new = precision_score(y_test_filtered_new, predictions_filtered_new, zero_division=0) * 100\n",
    "            recall_new = recall_score(y_test_filtered_new, predictions_filtered_new, zero_division=0) * 100\n",
    "            f1_new = f1_score(y_test_filtered_new, predictions_filtered_new, zero_division=0) * 100\n",
    "            specificity_new = (tn_new / (tn_new + fp_new)) * 100 if (tn_new + fp_new) > 0 else 0\n",
    "        else:\n",
    "            accuracy_new = precision_new = recall_new = f1_new = specificity_new = 0\n",
    "\n",
    "        metrics_dict[reject_threshold_new]['accuracy'].append(accuracy_new)\n",
    "        metrics_dict[reject_threshold_new]['precision'].append(precision_new)\n",
    "        metrics_dict[reject_threshold_new]['recall'].append(recall_new)\n",
    "        metrics_dict[reject_threshold_new]['f1'].append(f1_new)\n",
    "        metrics_dict[reject_threshold_new]['specificity'].append(specificity_new)\n",
    "        \n",
    "        metrics_table_data_new.append([round(reject_threshold_new, 2), f\"{accuracy_new:.2f}%\", f\"{precision_new:.2f}%\", f\"{recall_new:.2f}%\", f\"{f1_new:.2f}%\", f\"{specificity_new:.2f}%\"])\n",
    "\n",
    "        # Plot confusion matrix for this lambda\n",
    "        if cm_new.shape != (2, 2):\n",
    "            cm_padded_new = np.zeros((2, 2), dtype=int)\n",
    "            cm_padded_new[:cm_new.shape[0], :cm_new.shape[1]] = cm_new\n",
    "        else:\n",
    "            cm_padded_new = cm_new\n",
    "\n",
    "        x_labels_new = ['Normal', 'Abnormal']\n",
    "        y_labels_new = ['Abnormal', 'Normal']  # Reverse the order of y_labels\n",
    "        cm_reversed_new = cm_padded_new[::-1]\n",
    "        fig_new = ff.create_annotated_heatmap(z=cm_reversed_new, x=x_labels_new, y=y_labels_new, colorscale='Blues')\n",
    "        fig_new.update_layout(\n",
    "            title=f'Confusion Matrix, Fold {fold_number}, Lambda {reject_threshold_new:.2f}',\n",
    "            xaxis=dict(title='Predicted labels', tickfont=dict(size=10)),  # Adjust tick font size\n",
    "            yaxis=dict(title='True labels', tickfont=dict(size=10)),  # Adjust tick font size\n",
    "            width=400,  # Adjust width\n",
    "            height=300,  # Adjust height\n",
    "            margin=dict(l=50, r=50, t=130, b=50)  # Corrected margin specification\n",
    "        )\n",
    "        fig_new.show()\n",
    "\n",
    "    # Plot confusion matrix elements vs. lambda\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(lambdas, tp_list_new, marker='o', linestyle='-', label='True Positives (TP)')\n",
    "    plt.plot(lambdas, tn_list_new, marker='o', linestyle='-', label='True Negatives (TN)')\n",
    "    plt.plot(lambdas, fp_list_new, marker='o', linestyle='-', label='False Positives (FP)')\n",
    "    plt.plot(lambdas, fn_list_new, marker='o', linestyle='-', label='False Negatives (FN)')\n",
    "    plt.xlabel('Lambda (Abstain Threshold)')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title(f'Confusion Matrix Elements vs. Lambda Threshold, Fold {fold_number}')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Create a DataFrame for the table and display it\n",
    "    df_table_cm_new = pd.DataFrame(table_data_new, columns=['Lambda Threshold', 'True Negatives (TN)', 'False Positives (FP)', 'False Negatives (FN)', 'True Positives (TP)'])\n",
    "    fig_table_cm_new = go.Figure(data=[go.Table(\n",
    "        header=dict(values=list(df_table_cm_new.columns), fill_color='paleturquoise', align='left'),\n",
    "        cells=dict(values=[df_table_cm_new[col].tolist() for col in df_table_cm_new.columns], fill=dict(color=['lavender', 'white']), align='left')\n",
    "    )])\n",
    "    fig_table_cm_new.update_layout(width=1000, height=500)  # Adjust the size as needed to fit the table and ensure all entries are visible\n",
    "    fig_table_cm_new.show()\n",
    "\n",
    "    # Save the DataFrame to an Excel file\n",
    "    df_table_cm_new.to_excel(f'{save_directory}/Lambda_Abstain_Confusion_Matrix_Elements_Fold_{fold_number}.xlsx', index=False)\n",
    "  \n",
    "    # Create a DataFrame for abstain instances table and display it\n",
    "    df_abstain_table_new = pd.DataFrame(abstain_table_data_new, columns=['Lambda Threshold', 'Abstain Instances (Index, True Label)'])\n",
    "    df_abstain_table_new['Tested Instance'] = tested_instance_name\n",
    "    fig_abstain_table_new = go.Figure(data=[go.Table(\n",
    "        header=dict(values=list(df_abstain_table_new.columns), fill_color='paleturquoise', align='left'),\n",
    "        cells=dict(values=[df_abstain_table_new[col].tolist() for col in df_abstain_table_new.columns], fill=dict(color=['lavender', 'white']), align='left')\n",
    "    )])\n",
    "    fig_abstain_table_new.update_layout(width=1000, height=500)  # Adjust the size as needed to fit the table and ensure all entries are visible\n",
    "    fig_abstain_table_new.show()\n",
    "\n",
    "    # Save the abstain instances DataFrame to an Excel file\n",
    "    df_abstain_table_new.to_excel(f'{save_directory}/Lambda_Abstain_Instances_Fold_{fold_number}.xlsx', index=False)\n",
    "    \n",
    "    # Create a DataFrame for the performance metrics table and display it\n",
    "    df_metrics_table_new = pd.DataFrame(metrics_table_data_new, columns=['Lambda Threshold', 'Accuracy', 'Precision', 'Recall', 'F1 Score', 'Specificity'])\n",
    "    df_metrics_table_new['Tested Instance'] = tested_instance_name\n",
    "    fig_metrics_table_new = go.Figure(data=[go.Table(\n",
    "        header=dict(values=list(df_metrics_table_new.columns), fill_color='paleturquoise', align='left'),\n",
    "        cells=dict(values=[df_metrics_table_new[col].tolist() for col in df_metrics_table_new.columns], fill=dict(color=['lavender', 'white']), align='left')\n",
    "    )])\n",
    "    fig_metrics_table_new.update_layout(width=1000, height=500)  # Adjust the size as needed to fit the table and ensure all entries are visible\n",
    "    fig_metrics_table_new.show()\n",
    "\n",
    "    # Save the performance metrics DataFrame to an Excel file\n",
    "    df_metrics_table_new.to_excel(f'{save_directory}/Lambda_Abstain_Results_Metrics_Fold_{fold_number}.xlsx', index=False)\n",
    "\n",
    "    fold_number += 1\n",
    "\n",
    "# Calculate average metrics for each lambda across all folds\n",
    "avg_metrics_data_new = []\n",
    "for l in lambdas:\n",
    "    avg_accuracy_new = np.mean(metrics_dict[l]['accuracy'])\n",
    "    avg_precision_new = np.mean(metrics_dict[l]['precision'])\n",
    "    avg_recall_new = np.mean(metrics_dict[l]['recall'])\n",
    "    avg_f1_new = np.mean(metrics_dict[l]['f1'])\n",
    "    avg_specificity_new = np.mean(metrics_dict[l]['specificity'])\n",
    "    \n",
    "    avg_metrics_data_new.append([round(l, 2), f\"{avg_accuracy_new:.2f}%\", f\"{avg_precision_new:.2f}%\", f\"{avg_recall_new:.2f}%\", f\"{avg_f1_new:.2f}%\", f\"{avg_specificity_new:.2f}%\"])\n",
    "\n",
    "# Create a DataFrame for average metrics and display it\n",
    "df_avg_metrics_new = pd.DataFrame(avg_metrics_data_new, columns=['Lambda', 'Average Accuracy', 'Average Precision', 'Average Recall', 'Average F1-score', 'Average Specificity'])\n",
    "fig_avg_metrics_new = go.Figure(data=[go.Table(\n",
    "    header=dict(values=list(df_avg_metrics_new.columns), fill_color='paleturquoise', align='left'),\n",
    "    cells=dict(values=[df_avg_metrics_new[col].tolist() for col in df_avg_metrics_new.columns], fill=dict(color=['lavender', 'white']), align='left')\n",
    ")])\n",
    "fig_avg_metrics_new.update_layout(width=1000, height=500)  # Adjust the size as needed to fit the table and ensure all entries are visible\n",
    "fig_avg_metrics_new.show()\n",
    "\n",
    "# Save the average metrics DataFrame to an Excel file\n",
    "df_avg_metrics_new.to_excel(f'{save_directory}/Average_Metrics_Per_Lambda.xlsx', index=False)\n",
    "\n",
    "# Plot the average performance metrics vs. lambda\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df_avg_metrics_new['Lambda'], df_avg_metrics_new['Average Accuracy'].str.rstrip('%').astype(float), marker='o', linestyle='-', label='Average Accuracy')\n",
    "plt.plot(df_avg_metrics_new['Lambda'], df_avg_metrics_new['Average Precision'].str.rstrip('%').astype(float), marker='o', linestyle='-', label='Average Precision')\n",
    "plt.plot(df_avg_metrics_new['Lambda'], df_avg_metrics_new['Average Recall'].str.rstrip('%').astype(float), marker='o', linestyle='-', label='Average Recall')\n",
    "plt.plot(df_avg_metrics_new['Lambda'], df_avg_metrics_new['Average F1-score'].str.rstrip('%').astype(float), marker='o', linestyle='-', label='Average F1-score')\n",
    "plt.plot(df_avg_metrics_new['Lambda'], df_avg_metrics_new['Average Specificity'].str.rstrip('%').astype(float), marker='o', linestyle='-', label='Average Specificity')\n",
    "plt.xlabel('Lambda (Abstain Threshold)')\n",
    "plt.ylabel('Percentage')\n",
    "plt.title('Average Performance Metrics vs. Lambda Threshold')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nAverage metrics for each lambda have been saved to 'Average_Metrics_Per_Lambda.xlsx'.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
