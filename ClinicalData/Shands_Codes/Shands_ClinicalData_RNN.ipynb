{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a0d52de-4fa0-411b-b059-bf11eaecbbaf",
   "metadata": {},
   "source": [
    "* The LSTM code is so computationally expensice!\n",
    "* The datset is high demensional\n",
    "* PCA or Autoencoder is needed here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8a478c-46f3-4024-8806-a5fc86f4ba2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "# Step 1: Train and Save the Model on the Large Dataset\n",
    "# Load Data from CSV file\n",
    "data_path = r\"C:\\Users\\Jaber\\OneDrive - University of Florida\\Educational\\Chapters\\Chapter3_ClinicalData\\Datasets\\Shands_Combined_Final.csv\"\n",
    "save_directory = r\"C:\\Users\\Jaber\\OneDrive - University of Florida\\Educational\\Research\\FHRT\\PROJECT\\GitHub\\FHR_Project\\Codes\\ML_models\\Models\\All_Results\\RNN_Results\\TransferLearning_LSTM\"\n",
    "model_save_path = r'C:\\Users\\Jaber\\OneDrive - University of Florida\\Educational\\Research\\FHRT\\PROJECT\\GitHub\\FHR_Project\\Data_Results\\PreTrainedModel_RNN\\pre_trained_model.h5'\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Drop the first column (Sample names/identifiers)\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Treat missing values only for numeric columns using mean strategy\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].mean())\n",
    "\n",
    "# Extract features (all columns except the last column, which is 'label')\n",
    "X_time_series = df.iloc[:, :-50].values  # Exclude the last 50 columns, considering time series features\n",
    "X_additional_features = df[['Gestational age (GA)', 'Birthweight percentile for GA', 'Neonate sex', 'Maternal age',\n",
    "                            'Number of prior deliveries at >20 weeks gestations', 'Race/ethnicity', 'Chronic hypertension', \n",
    "                            'Type 1 diabetes mellitus', 'Type 2 diabetes mellitus', 'GHTN/Preeclampsia', 'Gestational diabetes', \n",
    "                            'Tobacco use', 'Alcohol use', 'Illicit drug use', 'Did patient require augmentation with pitocin?', \n",
    "                            'Indication for IOL - maternal hypertension', 'Indication for IOL - maternal diabetes', \n",
    "                            'Indication for IOL - late or post-term gestation', 'Indication for IOL - fetal growth restriction', \n",
    "                            'Indication for IOL - oligohydramnios', 'Indication for IOL - NRFHT', 'Indication for IOL - PROM', \n",
    "                            'Indication for IOL - ICP', 'Indication for IOL - elective', 'What was the maximum dose of pitocin?', \n",
    "                            'How long was the maximum dose administered? (hours)', 'Maximum maternal systolic blood pressure (mmHg)', \n",
    "                            'Minimum maternal systolic blood pressure (mmHg)', 'Maximum maternal diastolic blood pressure (mmHg)', \n",
    "                            'Minimum maternal diastolic blood pressure (mmHg)', 'Maximum maternal temperature (Celsius)', \n",
    "                            'Minimum maternal temperature (Celsius)', 'Meconium-stained fluid', 'Chorioamnionitis']].values\n",
    "\n",
    "y = df['label'].values  # Include the target variable y\n",
    "\n",
    "# Combine time series features and additional features\n",
    "X = np.concatenate([X_time_series, X_additional_features], axis=1)\n",
    "\n",
    "# K-Fold Cross Validation\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Define the range of lambda values (excluding 0.95)\n",
    "lambdas = np.arange(0.5, 0.95, 0.05)\n",
    "\n",
    "# Initialize dictionaries to store metrics for each lambda\n",
    "metrics_dict = {l: {'accuracy': [], 'precision': [], 'recall': [], 'f1': [], 'specificity': [], 'abstained_samples': []} for l in lambdas}\n",
    "\n",
    "# Function to classify based on probability and threshold\n",
    "def classify_with_reject(probabilities, threshold, initial_fp_fn_indices):\n",
    "    predictions = []\n",
    "    abstain_instances = []  # To store indices of abstain instances\n",
    "    for i, prob in enumerate(probabilities):\n",
    "        if i not in initial_fp_fn_indices:\n",
    "            if prob >= 0.5:\n",
    "                predictions.append(1)\n",
    "            else:\n",
    "                predictions.append(0)\n",
    "        else:\n",
    "            if prob >= threshold:\n",
    "                predictions.append(1)  # Classify as positive if prob is >= threshold\n",
    "            elif prob < 1 - threshold:\n",
    "                predictions.append(0)  # Classify as negative if prob is < 1 - threshold\n",
    "            else:\n",
    "                predictions.append(-1)  # Reject classification for uncertain predictions\n",
    "                abstain_instances.append(i)  # Record abstain instance index\n",
    "    return np.array(predictions), abstain_instances\n",
    "\n",
    "# Perform K-Fold Cross Validation\n",
    "fold_number = 1\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Extract time series and additional features for training\n",
    "    X_train_time_series = X_train[:, :-50]\n",
    "    X_train_additional = X_train[:, -50:]\n",
    "    \n",
    "    # Extract time series and additional features for testing\n",
    "    X_test_time_series = X_test[:, :-50]\n",
    "    X_test_additional = X_test[:, -50:]\n",
    "\n",
    "    # Standardize the features (mean=0, std=1)\n",
    "    scaler_time_series = StandardScaler()\n",
    "    X_train_time_series = scaler_time_series.fit_transform(X_train_time_series)\n",
    "    X_test_time_series = scaler_time_series.transform(X_test_time_series)\n",
    "\n",
    "    scaler_additional = StandardScaler()\n",
    "    X_train_additional = scaler_additional.fit_transform(X_train_additional)\n",
    "    X_test_additional = scaler_additional.transform(X_test_additional)\n",
    "\n",
    "    # Reshape data for LSTM input (samples, timesteps, features)\n",
    "    X_train_time_series = X_train_time_series.reshape((X_train_time_series.shape[0], X_train_time_series.shape[1], 1))\n",
    "    X_test_time_series = X_test_time_series.reshape((X_test_time_series.shape[0], X_test_time_series.shape[1], 1))\n",
    "\n",
    "    # No oversampling here, as oversampling was causing issues due to the balance of classes\n",
    "\n",
    "    # Define model inputs\n",
    "    input_time_series = tf.keras.layers.Input(shape=(X_train_time_series.shape[1], X_train_time_series.shape[2]))\n",
    "    input_additional = tf.keras.layers.Input(shape=(X_train_additional.shape[1],))\n",
    "\n",
    "    # Build LSTM layers\n",
    "    lstm1 = tf.keras.layers.LSTM(64, return_sequences=True)(input_time_series)\n",
    "    lstm2 = tf.keras.layers.LSTM(64)(lstm1)\n",
    "    flatten1 = tf.keras.layers.Flatten()(lstm2)\n",
    "\n",
    "    # Concatenate features and pass to Dense layers\n",
    "    concatenated_features = tf.keras.layers.concatenate([flatten1, input_additional])\n",
    "    dense_combined = tf.keras.layers.Dense(128, activation='relu')(concatenated_features)\n",
    "    dropout_combined = tf.keras.layers.Dropout(0.5)(dense_combined)\n",
    "    output = tf.keras.layers.Dense(1, activation='sigmoid')(dropout_combined)\n",
    "\n",
    "    # Compile and create the model\n",
    "    model = tf.keras.models.Model(inputs=[input_time_series, input_additional], outputs=output)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Train the model with early stopping\n",
    "    best_model = None\n",
    "    best_score = 0\n",
    "    no_improvement_epochs = 0\n",
    "    patience = 2  # Number of epochs with no improvement to wait before stopping early\n",
    "\n",
    "    for epoch in range(10):\n",
    "        history = model.fit([X_train_time_series, X_train_additional], y_train, epochs=1, batch_size=32, validation_split=0.2, verbose=0)\n",
    "\n",
    "        # Evaluate on the training data\n",
    "        train_accuracy = history.history['accuracy'][-1]\n",
    "        \n",
    "        # Check for early stopping\n",
    "        if train_accuracy > best_score:\n",
    "            best_model = model\n",
    "            best_score = train_accuracy\n",
    "            no_improvement_epochs = 0\n",
    "        else:\n",
    "            no_improvement_epochs += 1\n",
    "        \n",
    "        if no_improvement_epochs >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "            break\n",
    "\n",
    "    # Use the best model for predictions\n",
    "    test_probabilities = best_model.predict([X_test_time_series, X_test_additional]).flatten()  # Ensure it is 1D\n",
    "\n",
    "    # Initial classification with lambda=0.5\n",
    "    initial_predictions = (test_probabilities >= 0.5).astype(int)\n",
    "    cm_initial = confusion_matrix(y_test, initial_predictions, labels=[0, 1])\n",
    "    tn_initial, fp_initial, fn_initial, tp_initial = cm_initial.ravel()\n",
    "\n",
    "    initial_fp_fn_indices = [i for i in range(len(initial_predictions)) if initial_predictions[i] != y_test[i]]\n",
    "\n",
    "    # Initialize lists to store confusion matrix elements\n",
    "    tp_list = []\n",
    "    tn_list = []\n",
    "    fp_list = []\n",
    "    fn_list = []\n",
    "\n",
    "    # Initialize a table to store results for each lambda\n",
    "    table_data = []\n",
    "    abstain_table_data = []\n",
    "    metrics_table_data = []\n",
    "\n",
    "    # Loop through lambda values and calculate metrics\n",
    "    for reject_threshold in lambdas:\n",
    "        predictions, abstain_indices = classify_with_reject(test_probabilities, reject_threshold, initial_fp_fn_indices)\n",
    "        \n",
    "        # Filter out abstained instances\n",
    "        filtered_indices = [i for i in range(len(predictions)) if predictions[i] != -1]\n",
    "        y_test_filtered = y_test[filtered_indices]\n",
    "        predictions_filtered = predictions[filtered_indices]\n",
    "        \n",
    "        # Calculate confusion matrix elements\n",
    "        if len(predictions_filtered) > 0:\n",
    "            cm = confusion_matrix(y_test_filtered, predictions_filtered, labels=[0, 1])\n",
    "            tn, fp, fn, tp = cm.ravel()\n",
    "        else:\n",
    "            cm = np.array([[0, 0], [0, 0]])\n",
    "            tn, fp, fn, tp = 0, 0, 0, 0\n",
    "\n",
    "        # Append confusion matrix elements to lists\n",
    "        tp_list.append(tp)\n",
    "        tn_list.append(tn)\n",
    "        fp_list.append(fp)\n",
    "        fn_list.append(fn)\n",
    "        \n",
    "        # Append data to table\n",
    "        table_data.append([round(reject_threshold, 2), tn, fp, fn, tp])\n",
    "        \n",
    "        # Report abstain instances (rows of data)\n",
    "        abstain_instances_info = []\n",
    "        for idx in abstain_indices:\n",
    "            abstain_instances_info.append((idx, y_test[idx]))\n",
    "        \n",
    "        abstain_table_data.append([round(reject_threshold, 2), abstain_instances_info])\n",
    "\n",
    "        # Calculate and store metrics\n",
    "        if len(y_test_filtered) > 0:\n",
    "            accuracy = accuracy_score(y_test_filtered, predictions_filtered) * 100\n",
    "            precision = precision_score(y_test_filtered, predictions_filtered, zero_division=0) * 100\n",
    "            recall = recall_score(y_test_filtered, predictions_filtered, zero_division=0) * 100\n",
    "            f1 = f1_score(y_test_filtered, predictions_filtered, zero_division=0) * 100\n",
    "            specificity = (tn / (tn + fp)) * 100 if (tn + fp) > 0 else 0\n",
    "        else:\n",
    "            accuracy = precision = recall = f1 = specificity = 0\n",
    "\n",
    "        metrics_dict[reject_threshold]['accuracy'].append(accuracy)\n",
    "        metrics_dict[reject_threshold]['precision'].append(precision)\n",
    "        metrics_dict[reject_threshold]['recall'].append(recall)\n",
    "        metrics_dict[reject_threshold]['f1'].append(f1)\n",
    "        metrics_dict[reject_threshold]['specificity'].append(specificity)\n",
    "        metrics_dict[reject_threshold]['abstained_samples'].append(len(abstain_indices))\n",
    "        \n",
    "        metrics_table_data.append([round(reject_threshold, 2), f\"{accuracy:.2f}%\", f\"{precision:.2f}%\", f\"{recall:.2f}%\", f\"{f1:.2f}%\", f\"{specificity:.2f}%\"])\n",
    "\n",
    "        # Plot confusion matrix for this lambda\n",
    "        if cm.shape != (2, 2):\n",
    "            cm_padded = np.zeros((2, 2), dtype=int)\n",
    "            cm_padded[:cm.shape[0], :cm.shape[1]] = cm\n",
    "        else:\n",
    "            cm_padded = cm\n",
    "\n",
    "        x_labels = ['Normal', 'C-section']\n",
    "        y_labels = ['C-section', 'Normal']  # Reverse the order of y_labels\n",
    "        cm_reversed = cm_padded[::-1]\n",
    "        fig = ff.create_annotated_heatmap(z=cm_reversed, x=x_labels, y=y_labels, colorscale='Blues')\n",
    "        fig.update_layout(\n",
    "            title=f'Confusion Matrix, Fold {fold_number}, Lambda {reject_threshold:.2f}',\n",
    "            xaxis=dict(title='Predicted labels', tickfont=dict(size=10)),  # Adjust tick font size\n",
    "            yaxis=dict(title='True labels', tickfont=dict(size=10)),  # Adjust tick font size\n",
    "            width=400,  # Adjust width\n",
    "            height=300,  # Adjust height\n",
    "            margin=dict(l=50, r=50, t=130, b=50)  # Corrected margin specification\n",
    "        )\n",
    "        fig.show()\n",
    "\n",
    "    # Plot confusion matrix elements vs. lambda\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(lambdas, tp_list, marker='o', linestyle='-', label='True Positives (TP)')\n",
    "    plt.plot(lambdas, tn_list, marker='o', linestyle='-', label='True Negatives (TN)')\n",
    "    plt.plot(lambdas, fp_list, marker='o', linestyle='-', label='False Positives (FP)')\n",
    "    plt.plot(lambdas, fn_list, marker='o', linestyle='-', label='False Negatives (FN)')\n",
    "    plt.xlabel('Lambda (Abstain Threshold)')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Confusion Matrix Elements vs. Lambda Threshold')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Create a DataFrame for the table and display it\n",
    "    df_table_cm = pd.DataFrame(table_data, columns=['Lambda Threshold', 'True Negatives (TN)', 'False Positives (FP)', 'False Negatives (FN)', 'True Positives (TP)'])\n",
    "    fig_table_cm = go.Figure(data=[go.Table(\n",
    "        header=dict(values=list(df_table_cm.columns), fill_color='paleturquoise', align='left'),\n",
    "        cells=dict(values=[df_table_cm[col].tolist() for col in df_table_cm.columns], fill=dict(color=['lavender', 'white']), align='left')\n",
    "    )])\n",
    "    fig_table_cm.update_layout(width=1000, height=500)  # Adjust the size as needed to fit the table and ensure all entries are visible\n",
    "    fig_table_cm.show()\n",
    "\n",
    "    # Save the DataFrame to an Excel file\n",
    "    df_table_cm.to_excel(f'{save_directory}/Lambda_Abstain_Confusion_Matrix_Elements_Fold_{fold_number}.xlsx', index=False)\n",
    "  \n",
    "    # Create a DataFrame for abstain instances table and display it\n",
    "    df_abstain_table = pd.DataFrame(abstain_table_data, columns=['Lambda Threshold', 'Abstain Instances (Index, True Label)'])\n",
    "    fig_abstain_table = go.Figure(data=[go.Table(\n",
    "        header=dict(values=list(df_abstain_table.columns), fill_color='paleturquoise', align='left'),\n",
    "        cells=dict(values=[df_abstain_table[col].tolist() for col in df_abstain_table.columns], fill=dict(color=['lavender', 'white']), align='left')\n",
    "    )])\n",
    "    fig_abstain_table.update_layout(width=1000, height=500)  # Adjust the size as needed to fit the table and ensure all entries are visible\n",
    "    fig_abstain_table.show()\n",
    "\n",
    "    # Save the abstain instances DataFrame to an Excel file\n",
    "    df_abstain_table.to_excel(f'{save_directory}/Lambda_Abstain_Instances_Fold_{fold_number}.xlsx', index=False)\n",
    "    \n",
    "    # Create a DataFrame for the performance metrics table and display it\n",
    "    df_metrics_table = pd.DataFrame(metrics_table_data, columns=['Lambda Threshold', 'Accuracy', 'Precision', 'Recall', 'F1 Score', 'Specificity'])\n",
    "    fig_metrics_table = go.Figure(data=[go.Table(\n",
    "        header=dict(values=list(df_metrics_table.columns), fill_color='paleturquoise', align='left'),\n",
    "        cells=dict(values=[df_metrics_table[col].tolist() for col in df_metrics_table.columns], fill=dict(color=['lavender', 'white']), align='left')\n",
    "    )])\n",
    "    fig_metrics_table.update_layout(width=1000, height=500)  # Adjust the size as needed to fit the table and ensure all entries are visible\n",
    "    fig_metrics_table.show()\n",
    "\n",
    "    # Save the performance metrics DataFrame to an Excel file\n",
    "    df_metrics_table.to_excel(f'{save_directory}/Lambda_Abstain_Results_Metrics_Fold_{fold_number}.xlsx', index=False)\n",
    "\n",
    "    fold_number += 1\n",
    "\n",
    "# Save the final best model\n",
    "best_model.save(model_save_path)\n",
    "\n",
    "# Calculate average metrics for each lambda across all folds\n",
    "avg_metrics_data = []\n",
    "avg_abstained_samples = []\n",
    "for l in lambdas:\n",
    "    avg_accuracy = np.mean(metrics_dict[l]['accuracy'])\n",
    "    avg_precision = np.mean(metrics_dict[l]['precision'])\n",
    "    avg_recall = np.mean(metrics_dict[l]['recall'])\n",
    "    avg_f1 = np.mean(metrics_dict[l]['f1'])\n",
    "    avg_specificity = np.mean(metrics_dict[l]['specificity'])\n",
    "    avg_abstain = np.mean(metrics_dict[l]['abstained_samples'])\n",
    "    \n",
    "    avg_metrics_data.append([round(l, 2), f\"{avg_accuracy:.2f}%\", f\"{avg_precision:.2f}%\", f\"{avg_recall:.2f}%\", f\"{avg_f1:.2f}%\", f\"{avg_specificity:.2f}%\"])\n",
    "    avg_abstained_samples.append([round(l, 2), avg_abstain])\n",
    "\n",
    "# Create a DataFrame for average metrics and display it\n",
    "df_avg_metrics = pd.DataFrame(avg_metrics_data, columns=['Lambda', 'Average Accuracy', 'Average Precision', 'Average Recall', 'Average F1-score', 'Average Specificity'])\n",
    "fig_avg_metrics = go.Figure(data=[go.Table(\n",
    "    header=dict(values=list(df_avg_metrics.columns), fill_color='paleturquoise', align='left'),\n",
    "    cells=dict(values=[df_avg_metrics[col].tolist() for col in df_avg_metrics.columns], fill=dict(color=['lavender', 'white']), align='left')\n",
    ")])\n",
    "fig_avg_metrics.update_layout(width=800, height=500)  # Adjust the size as needed to fit the table and ensure all entries are visible\n",
    "fig_avg_metrics.show()\n",
    "\n",
    "# Save the average metrics DataFrame to an Excel file\n",
    "df_avg_metrics.to_excel(f'{save_directory}/Average_Metrics_Per_Lambda.xlsx', index=False)\n",
    "\n",
    "# Plot the average performance metrics vs. lambda\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(df_avg_metrics['Lambda'], df_avg_metrics['Average Accuracy'].str.rstrip('%').astype(float), marker='o', linestyle='-', label='Average Accuracy')\n",
    "plt.plot(df_avg_metrics['Lambda'], df_avg_metrics['Average Precision'].str.rstrip('%').astype(float), marker='o', linestyle='-', label='Average Precision')\n",
    "plt.plot(df_avg_metrics['Lambda'], df_avg_metrics['Average Recall'].str.rstrip('%').astype(float), marker='o', linestyle='-', label='Average Recall')\n",
    "plt.plot(df_avg_metrics['Lambda'], df_avg_metrics['Average F1-score'].str.rstrip('%').astype(float), marker='o', linestyle='-', label='Average F1-score')\n",
    "plt.plot(df_avg_metrics['Lambda'], df_avg_metrics['Average Specificity'].str.rstrip('%').astype(float), marker='o', linestyle='-', label='Average Specificity')\n",
    "plt.xlabel('Lambda (Abstain Threshold)')\n",
    "plt.ylabel('Percentage')\n",
    "plt.title('Average Performance Metrics vs. Lambda Threshold')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Create a DataFrame for average abstained samples and display it\n",
    "df_avg_abstained_samples = pd.DataFrame(avg_abstained_samples, columns=['Lambda', 'Average Abstained Samples'])\n",
    "fig_avg_abstained_samples = go.Figure(data=[go.Table(\n",
    "    header=dict(values=list(df_avg_abstained_samples.columns), fill_color='paleturquoise', align='left'),\n",
    "    cells=dict(values=[df_avg_abstained_samples[col].tolist() for col in df_avg_abstained_samples.columns], fill=dict(color=['lavender', 'white']), align='left')\n",
    ")])\n",
    "fig_avg_abstained_samples.update_layout(width=800, height=400)  # Adjust the size as needed to fit the table and ensure all entries are visible\n",
    "fig_avg_abstained_samples.show()\n",
    "\n",
    "# Save the average abstained samples DataFrame to an Excel file\n",
    "df_avg_abstained_samples.to_excel(f'{save_directory}/Average_Abstained_Samples_Per_Lambda.xlsx', index=False)\n",
    "\n",
    "# Plot the average number of abstained samples vs. lambda\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(df_avg_abstained_samples['Lambda'], df_avg_abstained_samples['Average Abstained Samples'], marker='o', linestyle='-', label='Average Abstained Samples')\n",
    "plt.xlabel('Lambda (Abstain Threshold)')\n",
    "plt.ylabel('Average Number of Abstained Samples')\n",
    "plt.title('Average Number of Abstained Samples vs. Lambda Threshold')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nAverage metrics and average abstained samples for each lambda across all folds have been saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08ec092-32e9-4e58-935e-881d9c18918c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642187b2-5423-411e-8db5-791287a0cad5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
